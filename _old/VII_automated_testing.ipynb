{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII. Automated Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import pytest\n",
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Assert Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that the random number x is positive\n",
    "\n",
    "x = random.random()\n",
    "\n",
    "# Solution\n",
    "assert x>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that the absolute difference between x and y is smaller than 1\n",
    "\n",
    "x = random.random()\n",
    "y = random.random()\n",
    "\n",
    "# Solution\n",
    "\n",
    "assert -1<(x-y)<1\n",
    "assert abs(x-y)<1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the assert statement so that x passes the assertion.\n",
    "\n",
    "x = False\n",
    "assert x\n",
    "\n",
    "# Solution\n",
    "\n",
    "assert x is False\n",
    "assert not x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that all elements in the list x are stings. When the assertion fails for any element x_i\n",
    "# the assertion shold print: \"x_i is not a string!\" (replace x_i with the actual value)\n",
    "\n",
    "x = [\"1\", \"2\", 3]\n",
    "\n",
    "# Solution\n",
    "for x_i in x:\n",
    "    assert isinstance(x_i, str), str(x_i) + \" is not a string!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that the config dictionary below contains the key \"numbers\" and that the value stored under that key is a list\n",
    "\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Solution\n",
    "assert \"numbers\" in config\n",
    "assert isinstance(config[\"numbers\"], list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that asserts a number is even.\n",
    "# Call this function with an input that passes the assert and with an input that fails the assert.\n",
    "\n",
    "# Solution\n",
    "\n",
    "def assert_even_len(x):\n",
    "    assert len(x)%2 == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asssert that the data frame loaded below contains 7 trials\n",
    "\n",
    "df = pd.read_csv(\"trials.csv\")\n",
    "\n",
    "# Solution\n",
    "assert len(df) == 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The day2 folder contains a file called `sequencegen.py` that contains functions to generate and\n",
    "# write trial sequences. There is also a file called `test_sequencegen.py` with tests for functions in\n",
    "# `sequencegen.py`. Open both files and identify for every function in `test_sequence.py`\n",
    "\n",
    "\n",
    "# test_sequence_has_correct_len -> make sequence\n",
    "# test_exceed_max_iterations -> make_sequence\n",
    "# test_unique_list_has_no_repetitions -> has_repetitions\n",
    "# test_repetition_is_only_counted_within_min_dist -> has_repetitions\n",
    "# test_ignore_repetitions -> has_repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing the cell will run pytest which, by default will search the current folder for any files\n",
    "# called test_someting and run every function inside them that starts with test_. Run pytest and read\n",
    "# the output --- how many tests were run and how many of those were  passed and failed respectively?\n",
    "# BONUS: Identify the cause of the failed test\n",
    "\n",
    "!python -m pytest --verbose\n",
    "\n",
    "# Solution\n",
    "# 11 tests, one of them failed because of the rounding error in make_sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For the remainder of this notebook we won't call pytest directly instead we are going to use the ipytest tag. Whenever we run a cell that starts with `%%ipytest`, pytest will automatically execute any function starting with `test_` in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the next exercises, we'll test the function below\n",
    "# Decribe what the function does in one sentence.\n",
    "\n",
    "def trial_sequence(conditions: list, n_reps: int, shuffle:bool = True):\n",
    "    trials = conditions * n_reps\n",
    "    if shuffle:\n",
    "        random.shuffle(trials)\n",
    "    return trials\n",
    "\n",
    "# Solution\n",
    "# generate a randomized trialsequence with equiprobable conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "# Write a test to check that the sequence returned by the `trial_sequence` function has the desired length.\n",
    "# Pick one value for each parameter, for example conditions = [1, 2, 3] and n_reps = 4.\n",
    "\n",
    "# Solution\n",
    "def test_trial_sequence_has_correct_len():\n",
    "    trials = trial_sequence([1,2,3], n_reps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "# Write two more test functions for `trial_sequence`. One to test that the sequence is \n",
    "# not shuffled when shuffle=False and one to test that it is shuffled when shuffle=True\n",
    "\n",
    "# Solution\n",
    "def test_trial_sequence_is_shuffled():\n",
    "    trials1 = trial_sequence(conditions=[1,2,3], n_reps=1000)\n",
    "    trials2 = trial_sequence(conditions=[1,2,3], n_reps=1000)\n",
    "    assert trials1 != trials2\n",
    "\n",
    "def test_trial_sequence_is_ordered():\n",
    "    trials1 = trial_sequence(conditions=[1,2,3], n_reps=1000, shuffle=False)\n",
    "    trials2 = trial_sequence(conditions=[1,2,3], n_reps=1000, shuffle=False)\n",
    "    assert trials1 == trials2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "# write a parameterized test for `trial_sequence` to test that the sequence has the correct length for different\n",
    "# numbers of repeats. Try to make the range of tested values as wide as possible (without notably decreasing performance).\n",
    "# Keep the list of `conditions` the same, for example [\"a\", \"b\"]\n",
    "\n",
    "# Solution\n",
    "@pytest.mark.parametrize(\"n_reps\", [0, 5, 10000])\n",
    "def test_trial_sequence_has_correct_len(n_reps):\n",
    "    conditions = [\"a\", \"b\", \"c\"]\n",
    "    n_trials = len(conditions)*n_reps\n",
    "    trials = trial_sequence(conditions, n_reps)\n",
    "    assert len(trials) == n_trials  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conditions': [1, 2, 3], 'n_trials': 20}\n"
     ]
    }
   ],
   "source": [
    "# Now we are going to test this function\n",
    "\n",
    "def load_config(fpath:str):\n",
    "    with open(fpath) as f:\n",
    "        config = json.load(f)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "# Write a test for `load_config` to test that the loaded config contains the keys \"conditions\" and \"n_trials\"\n",
    "\n",
    "# Solution\n",
    "def test_config_has_keys():\n",
    "    config = load_config(\"config.json\")\n",
    "    assert \"conditions\" in config\n",
    "    assert \"n_trials\" in config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "# Write a test for `load_config` to test that it raises a FileNotFoundError if we pass the path to a file that does not exist\n",
    "\n",
    "# Solution\n",
    "def test_load_config_raises_error():\n",
    "    with pytest.raises(FileNotFoundError):\n",
    "        load_config(\"comfig.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "# Write a test for `load_config` to test that the value stored under the key \"conditions\" is a list of integers\n",
    "\n",
    "# Solution\n",
    "def test_conditions_is_list_of_int():\n",
    "    config = load_config(\"config.json\")\n",
    "    assert isinstance(config[\"conditions\"], list)\n",
    "    for c in config[\"conditions\"]:\n",
    "        assert isinstance(c, int)\n",
    "    # OR\n",
    "    all([isinstance(c, int) for c in config[\"conditions\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Coverage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psychopy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
